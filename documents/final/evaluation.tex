\chapter{Critical Evaluation}
\section{Comparison of SwarmAvg to Other Methods}
Although SwarmAvg has demonstrated promising results, it remains important to conduct a comparative analysis against established methods. This will enable individuals seeking to incorporate distributed machine learning into their future projects to make and informed decisions whether SwarmAvg is the algorithm that they should use.

\subsection{Comparison of SwarmAvg to FedAvg}
SwarmAvg has the significant benefit of being more fault tolerant than FedAvg. This is because FedAvg has a central server, which means that the whole network would stop functioning if the central server is stopped. Not only this, but if a node drops its connection to the server, that node is effectively ignored from training, which has a negative effect on performance.
However, SwarmAvg has some significant drawbacks, one of which being that is has been shown to be slower to converge than FedAvg. Not only this, but SwarmAvg can create a significantly higher amount of network traffic than FedAvg or FL in general. This effect is maximised when every node is connected to every other node, meaning that the number of connections scale by $O(n^2)$, where $n$ is the number of nodes. FL will always have number of connections and therefore network traffic scale with $O(n)$. However, it is worth noting that SwarmAvg will consume less network traffic in less dense networks of nodes.

FedAvg is a simple algorithm than SwarmAvg, with fewer parameters to tune. This could be seen as both an advantage and a disadvantage. On the one hand, simpler algorithms require less tuning, and therefore less work to function optimally. However, a more complex algorithm enables more customisability for the specific problem, possibly allowing higher performance.

\subsection{Comparison of SwarmAvg to SwarmBC}
Despite the fact that this paper did not perform tests on SwarmBC, it is still possible to compare the theoretical advantages of both methods over one another, based on a comprehensive understanding of the inner workings of each.

One of the primary drawbacks of utilizing SwarmAvg as opposed to SwarmBC is the potential occurrence of divergent training, which arises when individual nodes or groups of nodes begin to learn significantly different models from each other during SwarmAvg execution. Consequently, when merging the models from these groups, they prove incompatible and cause a decrease in overall performance. This detrimental effect may persist throughout the entire training process, leading to significantly worse outcomes than anticipated. Several factors can lead to divergent training, with low settings for $\alpha$ and $\gamma$, coupled with a sparsely-connected network, being highly correlated with its occurrence during testing. SwarmBC does not suffer from this issue, as the blockchain ensures that all nodes agree on the same model. However, during testing, divergent training was a rare phenomenon, and its remedy was a simple parameter tuning process upon detection.

One drawback of utilizing the SwarmAvg algorithm, as opposed to SwarmBC, is the absence of security features. The utilization of blockchain technology enables the straightforward authentication of nodes utilizing different algorithms that are frequently well-documented and integrated into the blockchain framework. On the other hand, SwarmAvg lacks any built-in authentication mechanism. As a result, to employ it securely, a separate stand-alone authentication system would need to be implemented. Failure to utilize authentication could result in a malicious node gaining entry and compromising the training process.

The rationale behind the development of SwarmAvg was to establish a more agile and efficient SL algorithm in comparison to the existing technologies. SwarmAvg surpasses SwarmBC in terms of computational efficiency, as nodes are not mandated to verify transactions. In contrast, Blockchain involves a comparatively substantial overhead as opposed to simply averaging the models of a nodes neighbours. Consequently, SwarmBC necessitates higher processing power to operate for the same amount of training. This overhead can impede the speed of training on a resource-limited system, such as a swarm of drones, as a considerable amount of time is consumed in validating transactions, rather than performing training. Thus, SwarmAvg may be a more fitting choice than SwarmBC for swarms where processing power is a concern.

\section{Pitfalls of this Study}

\subsection{The Small Simulated Number of Nodes}
A significant concern regarding this study is the limited number of nodes simulated during testing. Several drawbacks of FL in comparison to SL, such as server bottlenecking, are not apparent when testing on a limited number of nodes. Although SwarmAvg may have the potential to be utilized for training in large swarms, this particular scenario has not yet been evaluated.

\subsection{The Lack of Testing Overheads}
In the course of testing the SwarmAvg algorithm, it was determined that the assessment of its performance should be based on training steps rather than time. Although it would have been preferable to evaluate performance in relation to time, this was not feasible due to limitations in resources. The GPUs utilized in this study are primarily optimized for running a single GPU program effectively, whereas this research required multiple nodes to perform training simultaneously. As a result, it was noted during testing that training times varied significantly, rendering time-based measurement impractical due to high levels of noise.

\subsection{The Lack of Simulated Internet Lag}
The algorithm proposed is designed for usage over the internet. However, the study solely evaluated its performance within a simulated environment. It is worth noting that a fundamental distinction between the simulated environment and the real world is the absence of a time gap between the sending of a model update by one node and its reception by another. The deliberate omission of this time gap in the simulation aimed to increase the speed of simulations. However, this critical aspect of the algorithm's functionality remains untested as a result.