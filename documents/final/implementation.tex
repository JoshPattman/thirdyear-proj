\section{Dataset and Machine Learning Model}
\subsection{Dataset}
Initially, the dataset utilized for experimentation was the MNIST dataset, which encompasses 60000 greyscale 28x28 labelled images of digits from 0 to 9. This dataset was selected for its simplicity, requiring no pre-processing or data cleaning prior to training, and due to its availability as a built-in component of the chosen machine learning framework, Keras.

However, upon implementation it was determined that this dataset was too simple for the application of machine learning, as a single node could reach near peak accuracy after a single epoch, rendering it ill-suited for swarm learning, an algorithm designed to function across multiple training epochs.

To address this issue, the MNIST dataset was replaced with MNIST-fashion, a drop-in replacement dataset containing 10 different items of clothing. MNIST-fashion is known to be more challenging \citeme [https://arxiv.org/abs/1708.07747]. To further increase the complexity of the problem, in several experiments each agent was only provided with a small subset of the entire dataset, resulting in less training per epoch, and therefore meaning that an agent would require more epochs to achieve the same performance.

\subsection{Model}
The Keras machine learning framework in Python was utilized to implement the model due to its reputation for being both simple and straightforward. All experiments made use of the same model, which is outlined in Appendix \ref{ap:model} and is a small convolutional neural network. The model was tested on the MNIST fashion dataset and was able to attain an accuracy score of above 90 percent when trained on the entire dataset; this result is on par with the accuracy reported in the original paper \citeme [https://arxiv.org/abs/1708.07747 (same as above)], making the model suitable for use.


\section{Federated Learning}
Federated learning was chosen for comparison of performance against swarm learning. In order to ensure fairness of the comparison, it was necessary to implement the federated learning algorithm from scratch, using the same frameworks and language as the swarm learning algorithm.
\subsection{Algorithm}
\subsection{Back-end Distributor}
\subsection{Evaluation}

\section{Prototype}
\subsection{Algorithm}
\subsection{Back-end Distributor}
\subsection{Evaluation}

\section{Final}
\subsection{Algorithm}
\subsection{Back-end Distributor}
when using push on train rather than pull on sync, the latest diffs are more preserved
\subsection{Evaluation}
