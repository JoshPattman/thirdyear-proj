\section{Dataset and Machine Learning Model}
\subsection{Dataset}
Initially, the dataset utilized for experimentation was the MNIST dataset, which encompasses 60000 greyscale 28x28 labelled images of digits from 0 to 9. This dataset was selected for its simplicity, requiring no pre-processing or data cleaning prior to training, and due to its availability as a built-in component of the chosen machine learning framework, Keras.

However, upon implementation it was determined that this dataset was too simple for the application of machine learning, as a single node could reach near peak accuracy after a single epoch, rendering it ill-suited for swarm learning, an algorithm designed to function across multiple training epochs.

To address this issue, the MNIST dataset was replaced with MNIST-fashion, a drop-in replacement dataset containing 10 different items of clothing. MNIST-fashion is known to be more challenging \citeme [https://arxiv.org/abs/1708.07747]. To further increase the complexity of the problem, in several experiments each agent was only provided with a small subset of the entire dataset, resulting in less training per epoch, and therefore meaning that an agent would require more epochs to achieve the same performance.

\subsection{Model}
The Keras machine learning framework in Python was utilized to implement the model due to its reputation for being both simple and straightforward. All experiments made use of the same model, which is outlined in Appendix \ref{ap:model} and is a small convolutional neural network. The model was tested on the MNIST fashion dataset and was able to attain an accuracy score of above 90 percent when trained on the entire dataset; this result is on par with the accuracy reported in the original paper \citeme [https://arxiv.org/abs/1708.07747 (same as above)], making the model suitable for use.


\section{Federated Learning}
Federated learning was chosen for comparison of performance against swarm learning. In order to ensure fairness of the comparison, it was necessary to implement the federated learning algorithm from scratch, using the same frameworks and language as the swarm learning algorithm.
\subsection{Algorithm}
\begin{itemize}
	\item FedAVG
	\item All nodes train at once
	\item Flatten model -> send to nodes -> Unflatten model
\end{itemize}
\subsection{Back-end}
\begin{itemize}
	\item Initially used REST
	\item REST is a waste as it performs exactly the same when measuring epochs on x axis, but it is much slower
	\item Switched to a bunch of thread safe queues
\end{itemize}
\subsection{Evaluation}
\begin{itemize}
	\item Simple form of fed learning
	\item Is an accurate model when measuring over epochs, but not over time
\end{itemize}

\section{Prototype}
\begin{itemize}
	\item The first prototype was created as a proof-of-concept
	\item It was not designed to be used in the final results section, but rather to be very modifiable
\end{itemize}
\subsection{Algorithm}
\begin{itemize}
	\item Very simplified form of algorithm described in design section
	\item no beta or gamma
	\item no training counter
	\item sync step uses models on demand, not cached models -> slower
\end{itemize}
\subsection{Evaluation}
\begin{itemize}
	\item Helpful for algorithm development
	\item Helped create many of the ideas described in design
	\item Far less performant than fed and final implementation
	\item Not discussed any further
\end{itemize}

\section{Final}
\begin{itemize}
	\item What was learnt from prototype 1 was taken into account for this
	\item Designed to be usable and extendable
	\item The implementation used in results section
\end{itemize}
\subsection{Algorithm}
\begin{itemize}
	\item As described in design section
	\item It was tested using push model before combination and push model after combination. Push before worked better so is used. Hypothesis: The difference in training that this node made is more preserved
\end{itemize}
\subsection{Back-end}
\begin{itemize}
	\item Same method as fed learning
\end{itemize}
\subsection{Evaluation}
\begin{itemize}
	\item Good implementation
	\item Not accurate for time based benchmarks though, as it uses queues instead of real web stuff
	\item This could be very easily changed due to the design of the code
\end{itemize}
