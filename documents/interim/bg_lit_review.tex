\chapter{Background}
\section{Background Research}
\subsection{Federated Learning}
% Introduce federated learning
Many modern machine learning algorithms require large volumes of diverse data to achieve optimal performance. Real-world data is often distributed among multiple organizations that are unable to share it with each other due to privacy regulations, which can hinder the ability of machine learning models to reach their full potential. Federated Learning (FL) \cite{survey_on_fed_learning} is a technique in machine learning that aims to train a single model using all available data across organizations without requiring any data to be shared among them. \\

% Intro on how fed learning works and different types
There are a multitude of published FL frameworks with different merits and drawbacks \cite{survey_on_fed_learning}[p.~12]. Federated Averaging (FedAvg) \cite{fed_learning} is a commonly used yet simple framework, which breaks training into iterations where three steps take place:
\begin{enumerate}
	\item A copy of the current model is sent to each agent
	\item Each agent performs some training with their copy of the model and their own private data
	\item The trained models from each agent are sent back to the server to aggregate into a global model
\end{enumerate}
The global model improves over time, and after a certain number of steps, the training is complete and the global model is the final output. \\

% Specify why federated learning is better than conventional learning
As it does not require data to be shared between agents, FL is naturally beneficially for privacy sensitive tasks compared to conventional machine learning where the data is aggregated in a central location \citeme. However, as FL performs training on multiple agents in parallel, it can make better use of available training resources in situations where processing power is distributed among multiple nodes \citeme. 

\subsection{Swarm Learning}
% Introduce swarm learning
Swarm learning (SL) \cite{swarm_learning} is a subcategory of FL which operates in a completely distributed and decentralised manner. In contrast to FL, SL methods usually use a blockchain based system to coordinate an agreed upon global model \citeme. Similarly to FL though, SL uses repeated iterations during training, but each agent does not have to perform iterations in a synchronised manner:
\begin{enumerate}
	\item An agent obtains a copy of the current model from the blockchain
	\item The agent performs some training with their copy of the model and their own private data
	\item The updated model is merged with the current model on the blockchain and sent back to the blockchain for other agents to use
\end{enumerate}
The model on the blockchain will improve over time, and after a certain number of steps the blockchain model is the final output. \\

% Specify why swarm learning is better than conventional learning or federated learning
SL exhibits many of the same benefits of FL over conventional learning, but it also improves upon FL in some aspects. The absence of a central server makes SL more resilient to failures than centralized FL approaches such as FedAvg \citeme. The lack of a leader election protocol also means that SL may be better suited to tasks where networks of agents are sparsely connected, as data would not need to be forwarded as frequently throughout the network, as is the case with distributed FL techniques \citeme. Additionally, the removal of the need for a server in SL reduces the likelihood of performance bottlenecks for very large networks.

\subsection{Challenges for Federated and Swarm Learning}
There are numerous challenges associated with implementing FL or SL compared to conventional learning systems. Much research has been done. However, many of the proposed algorithms for FL have not been adapted to work with or tested on SL.

\subsubsection{Training Overhead}
Since FL and SL require communication between multiple agents, there is an inherent overhead cost associated with data transfer time and synchronization of agents. Additionally, the centralization of FL can result in server bottlenecking. This is an area where SL offers an improvement over conventional FL. One potential solution to reduce bottlenecks is to divide the agents into groups, each with its own central server, and enable those servers to communicate with a core server \cite{multi_center_fed_learning}. This has the potential to reduce the burden on the central server and improve overall performance.

\subsubsection{Unevenly Distributed Labels and Features - Non-IID Data}
When a dataset is collected from various sources and locations, it will typically have a specific distribution of features and labels. However, the distribution of these features and labels may differ among the different sources from which it was collected. For example, a dataset containing security camera footage of car and bus accidents may have more of one type of accident at some locations than others (labels). The cameras at different locations may also have different brightness or contrast settings (features). This can negatively impact the training of SL and FL algorithms \citeme because different agents are effectively proposing solutions to different problems. There is a significant amount of research being conducted on this topic, with various approaches being proposed.

One approach is to partition agents into different clusters, where each member of a cluster communicates with the same server. The servers at the centre of each cluster will perform FL with their respective agent, but still synchronise the cluster model to all other cluster servers \cite{multi_center_fed_learning}.

A very different approach is to use batch normalisation layers on each agent \cite{fedbn}. This method is more focussed on non-iid features than labels, but is shown to have excellent results as opposed to a multitude of other FL techniques.

\subsubsection{Sparsely Connected Networks}
For FL, ideally every agent would be able to have direct communication with the central server. Similarly for SL in the best case, every agent would have direct communication with every other agent. These cases are rarely ever realised however. The affect of having a sparsely connected network is more detrimental to FL than SL mainly because FL needs server communication, but SL and blockchain have been proven to work in sparse settings. This can however still slow the training of SL \citeme.

\subsubsection{Data Transfer Limits}
In the real world, it is sometimes not possible to transfer entire models between agents on a regular basis, due to data transfer limits. It has been proposed that a possible solution could be to use FedAvg \cite{fed_learning}, but only transfer segments of the network at each training step \cite{gossip_learning}. This approach outperformed FedAvg by a factor of between 2.25 and 3.01 with respect to training speed, due to the removal of the central server bottleneck and reduced data transferred \cite{gossip_learning}[section.~5.2].

\subsubsection{Low Processing Power Agents}
FL and SL are promising techniques for robotic systems \cite{fed_in_robotics}. However, edge robotics often do not have the same high level of processing power as a central machine learning server. For this reason, it is imperative that FL and SL systems are as efficient as possible with the results of their training as to minimise the loss caused by reduction in processing power.
